### Pre-training on an NLP task for non-NLP tasks
12 August 2021

This paper looks at what happens when you train a language model, then fine tune a tiny fraction of the weights on a non language task. Surprisingly, this appears to be very effective.

Come and join the reading group where we will be discussing the research paper: "Pretrained Transformers as Universal Computation Engines" https://arxiv.org/abs/2103.05247

As always we encourage you to read the material before coming to the meetup. It is fine if you cant understand parts of the it or cannot finish it. The purpose of this meetup is to spend some time with the material beforehand so that you can comment about what you thought and ask questions around where you got stuck. The purpose of the meetup is to learn.

If you would like to propose a research paper, blog post, Kaggle competition or something else as the topic for the following meeting, prepare a 30-60 second proposal. We will have these proposals at the end of the session.

We hope to see you there!

Meeting Schedule:

6:30 - Introductions
6:35 - Discussion
7:25 - Nominate and vote for following meeting's next topic
7:30 - Meeting ends

[Meetup Event Link](https://www.meetup.com/Data-Science-Discussion-Auckland/events/278866779)
RSVPs: 25
Waiting: 0
Event Type: physical
